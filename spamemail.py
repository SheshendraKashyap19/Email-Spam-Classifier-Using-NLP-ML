# -*- coding: utf-8 -*-
"""SPAMemail

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rDR0dFGwNYN7cwFuMvqSpktGIqWb2Nk5
"""

!pip install scikit-learn

!pip install datasets transformers sklearn nltk pandas joblib

!pip install datasets transformers scikit-learn nltk pandas joblib

from datasets import load_dataset

# Load Enron Spam dataset
dataset = load_dataset("bvk/ENRON-spam")
dataset

import pandas as pd

df = pd.DataFrame(dataset['train'])
df.head()

df['text'] = df['Subject'].astype(str) + " " + df['Message'].astype(str)
df['label'] = df['Spam/Ham'].apply(lambda x: 1 if x == "spam" else 0)

# Keep only relevant columns
df = df[['text', 'label']]
df.head()

import pandas as pd

df = pd.DataFrame(dataset['train'])
print(df.columns)
df.head()

import pandas as pd

# Convert email column (list) to string
df['text'] = df['email'].apply(lambda x: " ".join(x) if isinstance(x, list) else str(x))

# Keep only text + label
df = df[['text', 'label']]
df.head()

import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

def preprocess(text):
    text = text.lower()
    text = re.sub(r"http\S+|www\S+|https\S+", "", text)
    text = re.sub(r"[^a-zA-Z0-9 ]", " ", text)

    tokens = word_tokenize(text)
    tokens = [lemmatizer.lemmatize(w) for w in tokens if w not in stop_words]

    return " ".join(tokens)

df['clean_text'] = df['text'].apply(preprocess)
df.head()

import nltk

# Download all required NLTK packages
nltk.download('punkt', quiet=True)
nltk.download('stopwords', quiet=True)
nltk.download('wordnet', quiet=True)
nltk.download('omw-1.4', quiet=True)  # required for lemmatizer

import nltk

nltk.download('punkt_tab')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')

import pandas as pd

df = pd.DataFrame(dataset['train'])
print(df.columns)
df.head()

# Replace 'email' with the actual column name from df.columns
df['text'] = df['email'].apply(lambda x: " ".join(x) if isinstance(x, list) else str(x))
df = df[['text', 'label']]
df.head()

text = " ".join(dataset['train'][0]['email'])

import pandas as pd

# Extract emails safely
texts = []
labels = []

for item in dataset['train']:
    # join list to string if needed
    email_text = " ".join(item['email']) if isinstance(item['email'], list) else str(item['email'])
    texts.append(email_text)
    labels.append(item['label'])

df = pd.DataFrame({'text': texts, 'label': labels})
df.head()

import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize

nltk.download('punkt_tab', quiet=True)
nltk.download('stopwords', quiet=True)
nltk.download('wordnet', quiet=True)
nltk.download('omw-1.4', quiet=True)

stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

def preprocess(text):
    text = text.lower()
    text = re.sub(r"http\S+|www\S+|https\S+", "", text)
    text = re.sub(r"[^a-zA-Z0-9 ]", " ", text)

    tokens = word_tokenize(text)
    tokens = [lemmatizer.lemmatize(w) for w in tokens if w not in stop_words]

    return " ".join(tokens)

df['clean_text'] = df['text'].apply(preprocess)
df.head()

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    df['clean_text'], df['label'], test_size=0.2, random_state=42
)

from sklearn.feature_extraction.text import TfidfVectorizer

tfidf = TfidfVectorizer(max_features=5000)
X_train_tfidf = tfidf.fit_transform(X_train)
X_test_tfidf = tfidf.transform(X_test)

from sklearn.naive_bayes import MultinomialNB

model = MultinomialNB()
model.fit(X_train_tfidf, y_train)

from sklearn.metrics import accuracy_score, classification_report

y_pred = model.predict(X_test_tfidf)
print("Accuracy:", accuracy_score(y_test, y_pred)*100)
print(classification_report(y_test, y_pred))

import joblib

joblib.dump(model, "email_spam_model.joblib")
joblib.dump(tfidf, "tfidf_vectorizer.joblib")

def predict_email(text):
    # preprocess input
    import re
    from nltk.corpus import stopwords
    from nltk.stem import WordNetLemmatizer
    from nltk.tokenize import word_tokenize

    stop_words = set(stopwords.words('english'))
    lemmatizer = WordNetLemmatizer()

    def preprocess(text):
        text = text.lower()
        text = re.sub(r"http\S+|www\S+|https\S+", "", text)
        text = re.sub(r"[^a-zA-Z0-9 ]", " ", text)
        tokens = word_tokenize(text)
        tokens = [lemmatizer.lemmatize(w) for w in tokens if w not in stop_words]
        return " ".join(tokens)

    clean = preprocess(text)
    vector = tfidf.transform([clean])
    result = model.predict(vector)[0]
    return "SPAM" if result == 1 else "genuine"

# Example
predict_email("congrats sheshendra kashyap you got an internship")

from sklearn.metrics import accuracy_score, classification_report

y_pred = model.predict(X_test_tfidf)
print("Accuracy:", accuracy_score(y_test, y_pred)*100)

!git init
!git add .
!git commit -m "Initial commit: Add email spam classifier project"
!git remote add origin https://github.com/YourUsername/YourRepoName.git
!git branch -M main
!git push -u origin main

!git config --global user.email "sheshendrakashyap06@gmail.com"
!git config --global user.name "SheshendraKashyap19"

!git init
!git add .
!git commit -m "Initial commit: Add email spam classifier project"
!git remote add origin https://github.com/SheshendraKashyap19/Email-Spam-Classifier-Using-NLP-ML.git
!git branch -M main
!git push -u origin main